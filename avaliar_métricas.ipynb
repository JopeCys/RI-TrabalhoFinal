{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1203123e",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca316c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91026cd0",
   "metadata": {},
   "source": [
    "Carregar variáveis globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "73a35b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r id_consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5eec1",
   "metadata": {},
   "source": [
    "Configurando LOGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "68a7393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout,\n",
    "                    format='[%(asctime)s]%(levelname)s(%(name)s): %(message)s')\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431449b",
   "metadata": {},
   "source": [
    "Carregando .jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "99f43b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-22 09:43:30,602]INFO(__main__): Arquivos carregados com sucesso.\n"
     ]
    }
   ],
   "source": [
    "with open('map_consultas_relevancias.json', 'r', encoding='utf-8') as f:\n",
    "        map_consultas_relevancias = json.load(f)\n",
    "with open('resultados_bm25.json', 'r', encoding='utf-8') as f:\n",
    "    resultados_bm25 = json.load(f)\n",
    "with open('resultados_tfidf.json', 'r', encoding='utf-8') as f:\n",
    "    resultados_tfidf = json.load(f)\n",
    "# CORREÇÃO: Alterado de 'resultados_sorl.json' para 'resultado_solr.json'\n",
    "with open('resultados_solr.json', 'r', encoding='utf-8') as f:\n",
    "    resultados_solr = json.load(f)\n",
    "LOGGER.info(\"Arquivos carregados com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7e3a83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_documentos_relevantes_verdadeiros(id_consulta, mapa_relevancias_completo):\n",
    "    # 1. Acessa o dicionário de relevâncias para a consulta específica\n",
    "    # O uso de .get() evita erros se o ID não existir\n",
    "    relevancias_da_consulta = mapa_relevancias_completo.get(str(id_consulta))\n",
    "\n",
    "    # 2. Se a consulta não existir no mapa, retorna uma lista vazia\n",
    "    if not relevancias_da_consulta:\n",
    "        return []\n",
    "\n",
    "    # 3. Cria uma lista apenas com os IDs dos documentos cujo score é 1 ou 2\n",
    "    documentos_relevantes = [\n",
    "        doc_id\n",
    "        for doc_id, score in relevancias_da_consulta.items()\n",
    "        if score in [1, 2]\n",
    "    ]\n",
    "    \n",
    "    # 4. Retorna a lista de documentos relevantes encontrados\n",
    "    return documentos_relevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d6e96",
   "metadata": {},
   "source": [
    "Verificar e retornar correspondência entre consultas relevância e resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f6e8dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retornar_correspondencia(resultados_modelo, map_consultas_relevancias):\n",
    "    # 1. Obtém os documentos relevantes verdadeiros\n",
    "    documentos_relevantes_verdadeiros_raw = obter_documentos_relevantes_verdadeiros(id_consulta, map_consultas_relevancias)\n",
    "\n",
    "    # 2. Padroniza os dois conjuntos de dados para string e remove espaços\n",
    "    # Padroniza os resultados do modelo\n",
    "    set_resultados_modelo = {str(item).strip() for item in resultados_modelo}\n",
    "    \n",
    "    # Padroniza os documentos relevantes\n",
    "    set_documentos_relevantes = {str(item).strip() for item in documentos_relevantes_verdadeiros_raw}\n",
    "\n",
    "    # 3. Encontra a intersecção entre os dois conjuntos agora padronizados\n",
    "    correspondencias = set_resultados_modelo.intersection(set_documentos_relevantes)\n",
    "\n",
    "    # 4. Retorna a lista de correspondências (acertos)\n",
    "    return list(correspondencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aec943",
   "metadata": {},
   "source": [
    "BM25 - Precision, Recall e F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0e2c6a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-22 09:43:30,650]INFO(__main__): Documentos Relevantes Verdadeiros: 103 encontrados\n",
      "[2025-07-22 09:43:30,652]INFO(__main__): Acertos BM25: 13\n",
      "[2025-07-22 09:43:30,653]INFO(__main__): Precisão BM25: 26.0 %\n",
      "[2025-07-22 09:43:30,654]INFO(__main__): Recall BM25: 12.62135922330097 %\n",
      "[2025-07-22 09:43:30,655]INFO(__main__): F1 Score BM25: 16.99346405228758 %\n"
     ]
    }
   ],
   "source": [
    "# Calcula Documentos relevantes verdadeiros\n",
    "documentos_relevantes_verdadeiros = len(obter_documentos_relevantes_verdadeiros(id_consulta, map_consultas_relevancias))\n",
    "LOGGER.info(f\"Documentos Relevantes Verdadeiros: {documentos_relevantes_verdadeiros} encontrados\")\n",
    "\n",
    "# Calcula acertos\n",
    "bm25_acertos = len(retornar_correspondencia(resultados_bm25, map_consultas_relevancias))\n",
    "LOGGER.info(f\"Acertos BM25: {bm25_acertos}\")\n",
    "\n",
    "# Calcula Precision\n",
    "precision_bm25 = bm25_acertos / len(resultados_bm25) * 100 if len(resultados_bm25) > 0 else 0\n",
    "LOGGER.info(f\"Precisão BM25: {precision_bm25} %\")\n",
    "\n",
    "# Calcula Recall\n",
    "recall_bm25 = bm25_acertos / documentos_relevantes_verdadeiros * 100 if documentos_relevantes_verdadeiros > 0 else 0\n",
    "LOGGER.info(f\"Recall BM25: {recall_bm25} %\")\n",
    "\n",
    "# Calcula F1 Score\n",
    "f1_bm25 = 2 * (precision_bm25 * recall_bm25) / (precision_bm25 + recall_bm25) if (precision_bm25 + recall_bm25) > 0 else 0\n",
    "LOGGER.info(f\"F1 Score BM25: {f1_bm25} %\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98380e71",
   "metadata": {},
   "source": [
    "TD-IDF - Precision, Recall e F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8cffbe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-22 09:43:30,702]INFO(__main__): Documentos Relevantes Verdadeiros: 103 encontrados\n",
      "[2025-07-22 09:43:30,703]INFO(__main__): Acertos TFIDF: 14\n",
      "[2025-07-22 09:43:30,704]INFO(__main__): Precisão TFIDF: 28.000000000000004 %\n",
      "[2025-07-22 09:43:30,705]INFO(__main__): Recall TFIDF: 13.592233009708737 %\n",
      "[2025-07-22 09:43:30,707]INFO(__main__): F1 Score TFIDF: 18.30065359477124 %\n"
     ]
    }
   ],
   "source": [
    "# Calcula Documentos relevantes verdadeiros\n",
    "documentos_relevantes_verdadeiros = len(obter_documentos_relevantes_verdadeiros(id_consulta, map_consultas_relevancias))\n",
    "LOGGER.info(f\"Documentos Relevantes Verdadeiros: {documentos_relevantes_verdadeiros} encontrados\")\n",
    "\n",
    "# Calcula acertos\n",
    "tfidf_acertos = len(retornar_correspondencia(resultados_tfidf, map_consultas_relevancias))\n",
    "LOGGER.info(f\"Acertos TFIDF: {tfidf_acertos}\")\n",
    "\n",
    "# Calcula Precision\n",
    "precision_tfidf = tfidf_acertos / len(resultados_tfidf) * 100 if len(resultados_tfidf) > 0 else 0\n",
    "LOGGER.info(f\"Precisão TFIDF: {precision_tfidf} %\")\n",
    "\n",
    "# Calcula Recall\n",
    "recall_tfidf = tfidf_acertos / documentos_relevantes_verdadeiros * 100 if documentos_relevantes_verdadeiros > 0 else 0\n",
    "LOGGER.info(f\"Recall TFIDF: {recall_tfidf} %\")\n",
    "\n",
    "# Calcula F1 Score\n",
    "f1_tfidf = 2 * (precision_tfidf * recall_tfidf) / (precision_tfidf + recall_tfidf) if (precision_tfidf + recall_tfidf) > 0 else 0\n",
    "LOGGER.info(f\"F1 Score TFIDF: {f1_tfidf} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a6cae1",
   "metadata": {},
   "source": [
    "Sorl - Precision, Recall e F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2c042686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-22 09:43:30,722]INFO(__main__): Documentos Relevantes Verdadeiros SOLR: 103 encontrados\n",
      "[2025-07-22 09:43:30,723]INFO(__main__): Acertos SOLR: 12\n",
      "[2025-07-22 09:43:30,724]INFO(__main__): Precisão SOLR: 24.0 %\n",
      "[2025-07-22 09:43:30,726]INFO(__main__): Recall SOLR: 11.650485436893204 %\n",
      "[2025-07-22 09:43:30,727]INFO(__main__): F1 Score SOLR: 15.686274509803921 %\n"
     ]
    }
   ],
   "source": [
    "# Calcula Documentos relevantes verdadeiros\n",
    "documentos_relevantes_verdadeiros = len(obter_documentos_relevantes_verdadeiros(id_consulta, map_consultas_relevancias))\n",
    "LOGGER.info(f\"Documentos Relevantes Verdadeiros SOLR: {documentos_relevantes_verdadeiros} encontrados\")\n",
    "\n",
    "# Calcula acertos\n",
    "sorl_acertos = len(retornar_correspondencia(resultados_solr, map_consultas_relevancias))\n",
    "LOGGER.info(f\"Acertos SOLR: {sorl_acertos}\")\n",
    "\n",
    "# Calcula Precision\n",
    "precision_sorl = sorl_acertos / len(resultados_solr) * 100 if len(resultados_solr) > 0 else 0\n",
    "LOGGER.info(f\"Precisão SOLR: {precision_sorl} %\")\n",
    "\n",
    "# Calcula Recall\n",
    "recall_sorl = sorl_acertos / documentos_relevantes_verdadeiros * 100 if documentos_relevantes_verdadeiros > 0 else 0\n",
    "LOGGER.info(f\"Recall SOLR: {recall_sorl} %\")\n",
    "\n",
    "# Calcula F1 Score\n",
    "f1_sorl = 2 * (precision_sorl * recall_sorl) / (precision_sorl + recall_sorl) if (precision_sorl + recall_sorl) > 0 else 0\n",
    "LOGGER.info(f\"F1 Score SOLR: {f1_sorl} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a30c23",
   "metadata": {},
   "source": [
    "Função para Calcular Precisão Interpolada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ca3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECALL_LEVELS = np.linspace(0.0, 1.0, 11)\n",
    "\n",
    "def calcular_precisao_interpolada(retrieved_ids, relevant_ids_set):\n",
    "    if not relevant_ids_set:\n",
    "        return [0.0] * 11\n",
    "\n",
    "    # Garante que a comparação seja feita entre os mesmos tipos (string)\n",
    "    retrieved_ids_str = {str(doc_id) for doc_id in retrieved_ids}\n",
    "    relevant_ids_set_str = {str(doc_id) for doc_id in relevant_ids_set}\n",
    "\n",
    "    precision_values = []\n",
    "    recall_values = []\n",
    "    relevant_found_count = 0\n",
    "    for i, doc_id in enumerate(retrieved_ids):\n",
    "        if str(doc_id) in relevant_ids_set_str:\n",
    "            relevant_found_count += 1\n",
    "            precision = relevant_found_count / (i + 1)\n",
    "            recall = relevant_found_count / len(relevant_ids_set_str)\n",
    "            precision_values.append(precision)\n",
    "            recall_values.append(recall)\n",
    "\n",
    "    interpolated_precision = []\n",
    "    for r_level in RECALL_LEVELS:\n",
    "        precisions_at_level = [p for r, p in zip(recall_values, precision_values) if r >= r_level]\n",
    "        interpolated_precision.append(max(precisions_at_level) if precisions_at_level else 0.0)\n",
    "        \n",
    "    return interpolated_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd105e",
   "metadata": {},
   "source": [
    "Cálculo das Curvas e Plotagem do Gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cbb32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter a lista de documentos relevantes (como strings)\n",
    "documentos_relevantes_set = set(obter_documentos_relevantes_verdadeiros(id_consulta, map_consultas_relevancias))\n",
    "\n",
    "# Dicionário para armazenar as curvas\n",
    "curvas_modelos = {\n",
    "    \"BM25\": calcular_precisao_interpolada(resultados_bm25, documentos_relevantes_set),\n",
    "    \"Vetorial (TF-IDF)\": calcular_precisao_interpolada(resultados_tfidf, documentos_relevantes_set),\n",
    "    \"Solr\": calcular_precisao_interpolada(resultados_solr, documentos_relevantes_set)\n",
    "}\n",
    "\n",
    "# Plotar o gráfico\n",
    "LOGGER.info(\"Plotando o gráfico de Precisão x Recall...\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for nome_modelo, curva in curvas_modelos.items():\n",
    "    plt.plot(RECALL_LEVELS, curva, marker='o', linestyle='-', label=nome_modelo)\n",
    "\n",
    "plt.title(f'Precisão x Recall Interpolado (Consulta {id_consulta})', fontsize=16)\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precisão', fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(RECALL_LEVELS)\n",
    "plt.yticks(np.linspace(0.0, 1.0, 11))\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xlim(-0.01, 1.01)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projeto_final_ri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
