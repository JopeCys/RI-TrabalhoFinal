{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce8c367",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 58,
=======
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 95,
>>>>>>> 850bb72305ec7cae285a725b02f2b7dd97a04364
>>>>>>> d53f647f9797cf684035e8b9e2aa4e2d1aea650a
   "id": "b9a421e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (c:\\Users\\jopec\\miniconda3\\envs\\projeto_final_ri\\Lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m corpora, models, similarities\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jopec\\miniconda3\\envs\\projeto_final_ri\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m4.3.2\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m     14\u001b[39m logger = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mgensim\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger.handlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jopec\\miniconda3\\envs\\projeto_final_ri\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jopec\\miniconda3\\envs\\projeto_final_ri\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[32m     16\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIndexedCorpus\u001b[39;00m(interfaces.CorpusABC):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jopec\\miniconda3\\envs\\projeto_final_ri\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCorpusABC\u001b[39;00m(utils.SaveLoad):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jopec\\miniconda3\\envs\\projeto_final_ri\\Lib\\site-packages\\gensim\\matutils.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlapack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'triu' from 'scipy.linalg' (c:\\Users\\jopec\\miniconda3\\envs\\projeto_final_ri\\Lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import os\n",
    "import shutil\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf5522",
   "metadata": {},
   "source": [
    "Carregar variáveis globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06ea0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r id_consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0604ce98",
   "metadata": {},
   "source": [
    "Configuração do Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d2823b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0e5fa6",
   "metadata": {},
   "source": [
    "Carregando .jsons"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 61,
=======
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 97,
>>>>>>> 850bb72305ec7cae285a725b02f2b7dd97a04364
>>>>>>> d53f647f9797cf684035e8b9e2aa4e2d1aea650a
   "id": "dcd859a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('map_documentos.json', 'r', encoding='utf-8') as f:\n",
    "    map_documentos = json.load(f)\n",
    "with open('map_consultas.json', 'r', encoding='utf-8') as f:\n",
    "    map_consultas = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4dbe45",
   "metadata": {},
   "source": [
    "Download de dependências do NLTK\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 62,
=======
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 98,
>>>>>>> 850bb72305ec7cae285a725b02f2b7dd97a04364
>>>>>>> d53f647f9797cf684035e8b9e2aa4e2d1aea650a
   "id": "204c6c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c557246",
   "metadata": {},
   "source": [
    "Indexar o map_documentos com Vetorial (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 63,
   "id": "aebf068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie uma lista com os textos dos documentos. A ordem será preservada.\n",
    "lista_de_documentos = list(map_documentos.values())\n",
    "# Crie o corpus tokenizado a partir dessa lista\n",
    "corpus_tokenizado = [doc.lower().split(\" \") for doc in lista_de_documentos]\n",
    "# Guardar os IDs dos documentos na mesma ordem para mapeamento posterior\n",
    "ids_documentos = list(map_documentos.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef26e3a",
   "metadata": {},
   "source": [
    "Construção do modelo Vetorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
=======
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 99,
>>>>>>> 850bb72305ec7cae285a725b02f2b7dd97a04364
>>>>>>> d53f647f9797cf684035e8b9e2aa4e2d1aea650a
   "id": "0523b003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 08:43:53 - INFO - Indexando o corpus com o modelo Vetorial (TF-IDF)...\n",
      "2025-07-22 08:43:53 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2025-07-22 08:43:53 - INFO - adding document #10000 to Dictionary<8953 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:54 - INFO - adding document #20000 to Dictionary<12182 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:54 - INFO - adding document #30000 to Dictionary<16741 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:54 - INFO - adding document #40000 to Dictionary<19925 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:54 - INFO - adding document #50000 to Dictionary<22221 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:54 - INFO - adding document #60000 to Dictionary<25505 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:54 - INFO - adding document #70000 to Dictionary<27839 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:54 - INFO - adding document #80000 to Dictionary<28564 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:54 - INFO - adding document #90000 to Dictionary<31049 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:55 - INFO - adding document #100000 to Dictionary<33145 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:55 - INFO - adding document #110000 to Dictionary<34903 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:55 - INFO - adding document #120000 to Dictionary<36260 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:55 - INFO - adding document #130000 to Dictionary<37415 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:55 - INFO - adding document #140000 to Dictionary<39071 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:55 - INFO - adding document #150000 to Dictionary<42428 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:55 - INFO - adding document #160000 to Dictionary<44373 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...>\n",
      "2025-07-22 08:43:55 - INFO - built Dictionary<44923 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...> from 162259 documents (total 1400387 corpus positions)\n",
      "2025-07-22 08:43:55 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<44923 unique tokens: ['ajepidem', 'environmental', 'epidemiology', 'john', 'snow']...> from 162259 documents (total 1400387 corpus positions)\", 'datetime': '2025-07-22T08:43:55.964927', 'gensim': '4.3.3', 'python': '3.12.8 | Intel Corporation | (main, Apr  2 2025, 09:07:08) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-07-22 08:43:57 - INFO - collecting document frequencies\n",
      "2025-07-22 08:43:57 - INFO - PROGRESS: processing document #0\n",
      "2025-07-22 08:43:57 - INFO - PROGRESS: processing document #10000\n",
      "2025-07-22 08:43:57 - INFO - PROGRESS: processing document #20000\n",
      "2025-07-22 08:43:57 - INFO - PROGRESS: processing document #30000\n",
      "2025-07-22 08:43:57 - INFO - PROGRESS: processing document #40000\n",
      "2025-07-22 08:43:57 - INFO - PROGRESS: processing document #50000\n",
      "2025-07-22 08:43:57 - INFO - PROGRESS: processing document #60000\n",
      "2025-07-22 08:43:57 - INFO - PROGRESS: processing document #70000\n",
      "2025-07-22 08:43:57 - INFO - PROGRESS: processing document #80000\n",
      "2025-07-22 08:43:57 - INFO - PROGRESS: processing document #90000\n",
      "2025-07-22 08:43:58 - INFO - PROGRESS: processing document #100000\n",
      "2025-07-22 08:43:58 - INFO - PROGRESS: processing document #110000\n",
      "2025-07-22 08:43:58 - INFO - PROGRESS: processing document #120000\n",
      "2025-07-22 08:43:58 - INFO - PROGRESS: processing document #130000\n",
      "2025-07-22 08:43:58 - INFO - PROGRESS: processing document #140000\n",
      "2025-07-22 08:43:58 - INFO - PROGRESS: processing document #150000\n",
      "2025-07-22 08:43:58 - INFO - PROGRESS: processing document #160000\n",
      "2025-07-22 08:43:58 - INFO - TfidfModel lifecycle event {'msg': 'calculated IDF weights for 162259 documents and 44923 features (1374738 matrix non-zeros)', 'datetime': '2025-07-22T08:43:58.224341', 'gensim': '4.3.3', 'python': '3.12.8 | Intel Corporation | (main, Apr  2 2025, 09:07:08) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'initialize'}\n",
      "2025-07-22 08:43:58 - INFO - starting similarity index under ./temp_vetorial_index\\shard\n",
      "2025-07-22 08:43:59 - INFO - PROGRESS: fresh_shard size=10000\n",
      "2025-07-22 08:44:00 - INFO - PROGRESS: fresh_shard size=20000\n",
      "2025-07-22 08:44:01 - INFO - PROGRESS: fresh_shard size=30000\n",
      "2025-07-22 08:44:01 - INFO - creating sparse index\n",
      "2025-07-22 08:44:01 - INFO - creating sparse matrix from corpus\n",
      "2025-07-22 08:44:01 - INFO - PROGRESS: at document #0/32768\n",
      "2025-07-22 08:44:01 - INFO - PROGRESS: at document #10000/32768\n",
      "2025-07-22 08:44:02 - INFO - PROGRESS: at document #20000/32768\n",
      "2025-07-22 08:44:02 - INFO - PROGRESS: at document #30000/32768\n",
      "2025-07-22 08:44:02 - INFO - created <32768x44923 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 308100 stored elements in Compressed Sparse Row format>\n",
      "2025-07-22 08:44:02 - INFO - creating sparse shard #0\n",
      "2025-07-22 08:44:02 - INFO - saving index shard to ./temp_vetorial_index\\shard.0\n",
      "2025-07-22 08:44:02 - INFO - SparseMatrixSimilarity lifecycle event {'fname_or_handle': './temp_vetorial_index\\\\shard.0', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-07-22T08:44:02.233740', 'gensim': '4.3.3', 'python': '3.12.8 | Intel Corporation | (main, Apr  2 2025, 09:07:08) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'saving'}\n",
      "2025-07-22 08:44:02 - INFO - saved ./temp_vetorial_index\\shard.0\n",
      "2025-07-22 08:44:02 - INFO - loading SparseMatrixSimilarity object from ./temp_vetorial_index\\shard.0\n",
      "2025-07-22 08:44:02 - INFO - SparseMatrixSimilarity lifecycle event {'fname': './temp_vetorial_index\\\\shard.0', 'datetime': '2025-07-22T08:44:02.274160', 'gensim': '4.3.3', 'python': '3.12.8 | Intel Corporation | (main, Apr  2 2025, 09:07:08) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'loaded'}\n",
      "2025-07-22 08:44:02 - INFO - PROGRESS: fresh_shard size=0\n",
      "2025-07-22 08:44:03 - INFO - PROGRESS: fresh_shard size=10000\n",
      "2025-07-22 08:44:04 - INFO - PROGRESS: fresh_shard size=20000\n",
      "2025-07-22 08:44:05 - INFO - PROGRESS: fresh_shard size=30000\n",
      "2025-07-22 08:44:05 - INFO - creating sparse index\n",
      "2025-07-22 08:44:05 - INFO - creating sparse matrix from corpus\n",
      "2025-07-22 08:44:05 - INFO - PROGRESS: at document #0/32768\n",
      "2025-07-22 08:44:05 - INFO - PROGRESS: at document #10000/32768\n",
      "2025-07-22 08:44:06 - INFO - PROGRESS: at document #20000/32768\n",
      "2025-07-22 08:44:06 - INFO - PROGRESS: at document #30000/32768\n",
      "2025-07-22 08:44:06 - INFO - created <32768x44923 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 327694 stored elements in Compressed Sparse Row format>\n",
      "2025-07-22 08:44:06 - INFO - creating sparse shard #1\n",
      "2025-07-22 08:44:06 - INFO - saving index shard to ./temp_vetorial_index\\shard.1\n",
      "2025-07-22 08:44:06 - INFO - SparseMatrixSimilarity lifecycle event {'fname_or_handle': './temp_vetorial_index\\\\shard.1', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-07-22T08:44:06.415319', 'gensim': '4.3.3', 'python': '3.12.8 | Intel Corporation | (main, Apr  2 2025, 09:07:08) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'saving'}\n",
      "2025-07-22 08:44:06 - INFO - saved ./temp_vetorial_index\\shard.1\n",
      "2025-07-22 08:44:06 - INFO - loading SparseMatrixSimilarity object from ./temp_vetorial_index\\shard.1\n",
      "2025-07-22 08:44:06 - INFO - SparseMatrixSimilarity lifecycle event {'fname': './temp_vetorial_index\\\\shard.1', 'datetime': '2025-07-22T08:44:06.462402', 'gensim': '4.3.3', 'python': '3.12.8 | Intel Corporation | (main, Apr  2 2025, 09:07:08) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'loaded'}\n",
      "2025-07-22 08:44:06 - INFO - PROGRESS: fresh_shard size=0\n",
      "2025-07-22 08:44:07 - INFO - PROGRESS: fresh_shard size=10000\n",
      "2025-07-22 08:44:07 - INFO - PROGRESS: fresh_shard size=20000\n",
      "2025-07-22 08:44:08 - INFO - PROGRESS: fresh_shard size=30000\n",
      "2025-07-22 08:44:08 - INFO - creating sparse index\n",
      "2025-07-22 08:44:08 - INFO - creating sparse matrix from corpus\n",
      "2025-07-22 08:44:08 - INFO - PROGRESS: at document #0/32768\n",
      "2025-07-22 08:44:08 - INFO - PROGRESS: at document #10000/32768\n",
      "2025-07-22 08:44:09 - INFO - PROGRESS: at document #20000/32768\n",
      "2025-07-22 08:44:09 - INFO - PROGRESS: at document #30000/32768\n",
      "2025-07-22 08:44:09 - INFO - created <32768x44923 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 228785 stored elements in Compressed Sparse Row format>\n",
      "2025-07-22 08:44:09 - INFO - creating sparse shard #2\n",
      "2025-07-22 08:44:09 - INFO - saving index shard to ./temp_vetorial_index\\shard.2\n",
      "2025-07-22 08:44:09 - INFO - SparseMatrixSimilarity lifecycle event {'fname_or_handle': './temp_vetorial_index\\\\shard.2', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-07-22T08:44:09.192764', 'gensim': '4.3.3', 'python': '3.12.8 | Intel Corporation | (main, Apr  2 2025, 09:07:08) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'saving'}\n",
      "2025-07-22 08:44:09 - INFO - saved ./temp_vetorial_index\\shard.2\n",
      "2025-07-22 08:44:09 - INFO - loading SparseMatrixSimilarity object from ./temp_vetorial_index\\shard.2\n",
      "2025-07-22 08:44:09 - INFO - SparseMatrixSimilarity lifecycle event {'fname': './temp_vetorial_index\\\\shard.2', 'datetime': '2025-07-22T08:44:09.214999', 'gensim': '4.3.3', 'python': '3.12.8 | Intel Corporation | (main, Apr  2 2025, 09:07:08) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'loaded'}\n",
      "2025-07-22 08:44:09 - INFO - PROGRESS: fresh_shard size=0\n",
      "2025-07-22 08:44:10 - INFO - PROGRESS: fresh_shard size=10000\n",
      "2025-07-22 08:44:10 - INFO - PROGRESS: fresh_shard size=20000\n",
      "2025-07-22 08:44:11 - INFO - PROGRESS: fresh_shard size=30000\n",
      "2025-07-22 08:44:11 - INFO - creating sparse index\n",
      "2025-07-22 08:44:11 - INFO - creating sparse matrix from corpus\n",
      "2025-07-22 08:44:11 - INFO - PROGRESS: at document #0/32768\n",
      "2025-07-22 08:44:11 - INFO - PROGRESS: at document #10000/32768\n",
      "2025-07-22 08:44:11 - INFO - PROGRESS: at document #20000/32768\n",
      "2025-07-22 08:44:11 - INFO - PROGRESS: at document #30000/32768\n",
      "2025-07-22 08:44:11 - INFO - created <32768x44923 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 253951 stored elements in Compressed Sparse Row format>\n",
      "2025-07-22 08:44:11 - INFO - creating sparse shard #3\n",
      "2025-07-22 08:44:11 - INFO - saving index shard to ./temp_vetorial_index\\shard.3\n",
      "2025-07-22 08:44:11 - INFO - SparseMatrixSimilarity lifecycle event {'fname_or_handle': './temp_vetorial_index\\\\shard.3', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-07-22T08:44:11.857247', 'gensim': '4.3.3', 'python': '3.12.8 | Intel Corporation | (main, Apr  2 2025, 09:07:08) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'saving'}\n",
      "2025-07-22 08:44:11 - INFO - saved ./temp_vetorial_index\\shard.3\n",
      "2025-07-22 08:44:11 - INFO - loading SparseMatrixSimilarity object from ./temp_vetorial_index\\shard.3\n",
      "2025-07-22 08:44:11 - INFO - SparseMatrixSimilarity lifecycle event {'fname': './temp_vetorial_index\\\\shard.3', 'datetime': '2025-07-22T08:44:11.901811', 'gensim': '4.3.3', 'python': '3.12.8 | Intel Corporation | (main, Apr  2 2025, 09:07:08) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'loaded'}\n",
      "2025-07-22 08:44:11 - INFO - PROGRESS: fresh_shard size=0\n",
      "2025-07-22 08:44:12 - INFO - PROGRESS: fresh_shard size=10000\n",
      "2025-07-22 08:44:13 - INFO - PROGRESS: fresh_shard size=20000\n",
      "2025-07-22 08:44:14 - INFO - PROGRESS: fresh_shard size=30000\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Indexando o corpus com o modelo Vetorial (TF-IDF)...\")\n",
    "dictionary = corpora.Dictionary(corpus_tokenizado)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in corpus_tokenizado]\n",
    "tfidf_model = models.TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf_model[bow_corpus]\n",
    "\n",
    "# Usando 'similarities.Similarity' para eficiência de memória\n",
    "temp_folder = './temp_vetorial_index'\n",
    "if os.path.exists(temp_folder):\n",
    "    shutil.rmtree(temp_folder)\n",
    "os.makedirs(temp_folder)\n",
    "output_prefix = os.path.join(temp_folder, 'shard')\n",
    "index = similarities.Similarity(output_prefix, tfidf_corpus, num_features=len(dictionary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4da96e",
   "metadata": {},
   "source": [
    "Função que retorna Top N documentos de uma consulta Vetorial"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 65,
=======
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 100,
>>>>>>> 850bb72305ec7cae285a725b02f2b7dd97a04364
>>>>>>> d53f647f9797cf684035e8b9e2aa4e2d1aea650a
   "id": "01b69aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_top_n_ids_vetorial(consulta_tokenizada, n):\n",
    "    # 1. Converte a consulta para o espaço vetorial TF-IDF\n",
    "    query_bow = dictionary.doc2bow(consulta)\n",
    "    query_tfidf = tfidf_model[query_bow]\n",
    "\n",
    "    # 2. Calcula os scores (similaridades) para cada documento no corpus\n",
    "    doc_scores = index[query_tfidf]\n",
    "\n",
    "    # 3. Pega os índices dos scores em ordem decrescente de pontuação\n",
    "    top_n_indices = np.argsort(doc_scores)[::-1]\n",
    "\n",
    "    # 4. Retorna apenas os 'n' primeiros IDs da lista ordenada\n",
    "    # Pega o ID original da lista 'ids_documentos' e converte para inteiro\n",
    "    return [int(ids_documentos[idx]) for idx in top_n_indices[:n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3c913",
   "metadata": {},
   "source": [
    "Executar consulta e gerar os resultados top 50"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 101,
>>>>>>> 850bb72305ec7cae285a725b02f2b7dd97a04364
>>>>>>> d53f647f9797cf684035e8b9e2aa4e2d1aea650a
   "id": "e6445e54",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'consulta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m consulta_tokenizada = consulta_texto.lower().split(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Chame a função, que retornará os IDs dos documentos mais relevantes.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m resultados_vetorial = \u001b[43mobter_top_n_ids_vetorial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconsulta_tokenizada\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mobter_top_n_ids_vetorial\u001b[39m\u001b[34m(consulta_tokenizada, n)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobter_top_n_ids_vetorial\u001b[39m(consulta_tokenizada, n):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# 1. Converte a consulta para o espaço vetorial TF-IDF\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     query_bow = dictionary.doc2bow(\u001b[43mconsulta\u001b[49m)\n\u001b[32m      4\u001b[39m     query_tfidf = tfidf_model[query_bow]\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# 2. Calcula os scores (similaridades) para cada documento no corpus\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'consulta' is not defined"
     ]
    }
   ],
   "source": [
    "# Escolha uma consulta para testar\n",
<<<<<<< HEAD
    "consulta_texto = map_consultas[id_consulta]\n",
    "consulta_tokenizada = consulta_texto.lower().split(\" \")\n",
    "# Chame a função, que retornará os IDs dos documentos mais relevantes.\n",
    "resultados_vetorial = obter_top_n_ids_vetorial(consulta_tokenizada, n=50)"
=======
    "id_consulta_exemplo = \"1\" # Usando a primeira consulta como exemplo\n",
    "texto_consulta_exemplo = map_consultas[id_consulta_exemplo]\n",
    "\n",
    "print(f\"Texto da consulta (ID: {id_consulta_exemplo}): '{texto_consulta_exemplo}'\")\n",
    "\n",
    "# Tokeniza a consulta\n",
    "consulta_tokenizada = texto_consulta_exemplo.lower().split()\n",
    "\n",
    "# Top 50 documentos mais relevantes para a consulta\n",
    "resultados_vetorial = obter_top_n_vetorial(consulta_tokenizada, n=50)\n",
    "\n",
    "logging.info(f\"Encontrados {len(resultados_vetorial)} resultados.\")"
>>>>>>> d53f647f9797cf684035e8b9e2aa4e2d1aea650a
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf5816c",
   "metadata": {},
   "source": [
    "Salvando resultados_vetorial como .json"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 102,
>>>>>>> 850bb72305ec7cae285a725b02f2b7dd97a04364
>>>>>>> d53f647f9797cf684035e8b9e2aa4e2d1aea650a
   "id": "6313f99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 08:07:47 - INFO - Salvando resultados em 'resultados_vetorial.json'...\n",
      "2025-07-22 08:07:47 - INFO - Arquivo de resultados salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "nome_arquivo_saida = 'resultados_vetorial.json'\n",
    "logging.info(f\"Salvando resultados em '{nome_arquivo_saida}'...\")\n",
    "try:\n",
    "    with open(nome_arquivo_saida, 'w', encoding='utf-8') as out_file_opened:\n",
    "        json.dump(\n",
    "            resultados_vetorial,    # Dados a serem salvos\n",
    "            out_file_opened,        # Objeto do arquivo\n",
    "            indent=2,               # Indentação para melhorar legibilidade\n",
    "            ensure_ascii=False      # Permite caracteres não-ASCII\n",
    "        )\n",
    "    logging.info(\"Arquivo de resultados salvo com sucesso!\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erro ao salvar o arquivo JSON: {e}\")\n",
    "finally:\n",
    "    # Limpa a pasta temporária do índice\n",
    "    if os.path.exists(temp_folder):\n",
    "        shutil.rmtree(temp_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projeto_final_ri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
